# -*- coding: utf-8 -*-
import pandas as pd
from filterSentenceByVerb.assign_features import write
from filterSentenceByVerb.get_nmod_of_entities import *
from filterSentenceByVerb.convert_word_format import *
from filterSentenceByVerb.request_co_reference import *
from filterSentenceByVerb.get_verb_entities import *

collection = ["provide", "disclosure", "protect", "process", "utilize", "prohibit", "disclose", "distribute",
              "exchange", "give", "rent", "report", "sell", "send", "share", "trade", "transfer", "transmit", "against",
              "connect", "associate", "post", "combine", "lease", "sublicense", "expose", "afford", "resell", "tell",
              "deliver", "disseminate", "transport", "access", "collect", "gather", "know", "obtain", "receive", "save",
              "store", "use", "Keep", "proxy", "request", "track", "aggregate", "build", "augment", "cache",
              "republish", "get", "seek", "possess", "accumulate", "keep", "convert", "translate", "display", "add"]


def main():
    xlsx_list = get_raw_file()
    print(xlsx_list)
    for filename in xlsx_list:
        to_filemame = "../raw data/40_post_processed_data/" + filename.split("40_pre_processed_data/")[1]
        print("===============================" + str(to_filemame))
        if os.path.exists(to_filemame):
            continue
        get_result_file(filename)


    '''
    #sentence = "We may limit or remove your access to Messenger if you receive large amounts of negative feedback or violate our policies, as determined by us in our sole discretion."
    sentence = "you can monitor or collect data related to your use of SDKs."
    predictor = predictor = Predictor.from_path("https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz")
    nlp = spacy.load("en_core_web_sm")
    #phrase_set = ["data"]
    co_refence_predictor = Predictor.from_path("https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz")

    #print(assign_verb_and_entity_with_filter(sentence,predictor,nlp,phrase_set))
    paragraph = "We respect the Intellectual Property Rights of others and we expect our users to do the same. We will respond to clear notices of copyright infringement consistent with the Digital Millennium Copyright Act (“DMCA”). You can learn more about Adobe's IP Takedown policies and practices here: http://www.adobe.com/legal/dmca.html."
    print(get_co_reference(paragraph,co_refence_predictor))
    #print(is_first_party(sentence,nlp,predictor,verb))
    #standford_nlp = stanza.Pipeline()
    #print(assign_nmod_of_sensitive_word(sentence,standford_nlp,predictor))
    '''


def get_raw_file():
    xlsx_list = glob.glob('../raw data/40_pre_processed_data/*.xlsx')  # get all the filenames of the prepossed 40 toses
    print(u'have found %s xlsx files' % len(xlsx_list))
    print(u'正在处理............')
    return xlsx_list


def get_result_file(filename):
    # filename = "../data/raw data/facebook.xlsx"
    predictor = Predictor.from_path(
        "https://s3-us-west-2.amazonaws.com/allennlp/models/elmo-constituency-parser-2018.03.14.tar.gz")
    co_refence_predictor = Predictor.from_path(
        "https://s3-us-west-2.amazonaws.com/allennlp/models/coref-model-2018.02.05.tar.gz")

    nlp = spacy.load("en_core_web_sm")
    standford_nlp = stanza.Pipeline()
    sentence_list, align_paragraph, score = read_data(filename)

    # get verb,entities with filter
    verb_entity_list_with_filter = []
    # get verb,entities without filter
    verb_entity_list_without_filter = []
    # get nmod,entities with filter
    nmod_entity_list = []
    # get the subject of the verb
    verb_subject_list = []
    # co_reference relationship
    co_reference_list = []

    # get the subject of the co_reference
    subject_co_reference_list = []

    for index, sentence in enumerate(sentence_list):
        policy = policy_verb_entity(sentence, align_paragraph[index],predictor,co_refence_predictor,nlp,standford_nlp)

        phrase_set, sensitive_data = extractEntity(sentence, predictor)
        co_reference = get_co_reference(align_paragraph[index], co_refence_predictor)
        verb_entitiy_string_without_filter = assign_verb_and_entity_without_filter(sentence, predictor, nlp, phrase_set)
        verb_entity_list_without_filter.append(verb_entitiy_string_without_filter)

        verb_entitiy_string, flag, subject_list = assign_verb_and_entity_with_filter(sentence, predictor, nlp,
                                                                                     phrase_set, co_reference)
        verb_entity_list_with_filter.append(verb_entitiy_string)

        verb_subject_list.append(subject_list)
        co_reference_list.append(co_reference)

        subject_co_reference = get_subject_co_reference(subject_list, co_reference)
        subject_co_reference_list.append(subject_co_reference)

        if flag:
            nmod_entity_list.append("")
        else:
            nmod_entitiy_string = assign_nmod_of_sensitive_word(sentence, nlp, standford_nlp, predictor, sensitive_data)
            nmod_entity_list.append(nmod_entitiy_string)

    # construct the label of the sentence
    # if the value in verb_entity_list_with_filter and nmod_entity_list is not null, we label 1, otherwise label 0
    predict_label = []
    for i in range(0, len(sentence_list)):
        if len(verb_entity_list_with_filter[i]) != 0 or len(nmod_entity_list[i]) != 0:
            predict_label.append(1)
        else:
            predict_label.append(0)

    data = {}
    data["sentence_list"] = sentence_list
    data["align_paragraph"] = align_paragraph
    data["subject_co_reference"] = subject_co_reference_list
    data["co_reference_list"] = co_reference_list
    data["verb_subject_list"] = verb_subject_list

    data["score"] = score
    data["predict_label"] = predict_label
    data["verb_entity_list_without_filter"] = verb_entity_list_without_filter
    data["verb_entity_list_with_filter"] = verb_entity_list_with_filter
    data["nmod_entity_list"] = nmod_entity_list
    df = pd.DataFrame(data)
    to_filemame = "../raw data/40_post_processed_data/" + filename.split("40_pre_processed_data/")[1]
    df.to_excel(to_filemame, index=False, encoding="utf8",
                header=["sentence_list", "align_paragraph", "subject_co_reference", "co_reference_list",
                        "verb_subject_list", "score", "predict_label", "verb_entity_list_without_filter",
                        "verb_entity_list_with_filter", "nmod_entity_list"])


"it --> [your app, application] "
def get_subject_co_reference(subject_list, co_reference):
    string = ""
    for subject in subject_list:
        for pair in co_reference:
            if subject in pair:
                string = string + subject + "-->" + str(pair) + "\n"
    return string


def get_co_reference(paragraph, co_refence_predictor):
    # print("=========================")
    # print(paragraph)

    # results = co_refence_predictor.predict(paragraph)
    try:
        results = json.loads(request_co_reference(paragraph))
    except:
        return [[]]

    # print("111111111111111111")
    clusters = results["clusters"]
    relationship = []
    for cluster in clusters:
        pair = []
        for item in cluster:
            if item[0] == item[1]:
                # print(results["document"][item[0]])
                pair.append(results["document"][item[0]])
            else:
                # print(' '.join(results["document"][item[0]:item[1]]))
                pair.append(' '.join(results["document"][item[0]:item[1]]))
        relationship.append(pair)
    return relationship


# read raw data
def read_data(filename):
    sheet = pd.read_excel(filename)
    return sheet["sentence_list"].tolist(), sheet["align_paragraph"].tolist(), sheet["score"].tolist()


def assign_verb_and_entity_with_filter(sentence, predictor, nlp, phrase_set, co_reference):
    # sentence = "We also collect contact information if you choose to upload, sync or import it from a device such as an address book or call log or SMS log history, which we use for things like helping you and others find people you may know and for the other purposes listed below."
    ###condition 1: if the sentence is end with "?", return null string, and True which means it will not process nmod
    if sentence.strip().endswith("?"):
        return "", True, []

    ###condition 4: if the sentences contains give us, grant us, we return null string, and True which means it will not process nmod
    ###this condition can not be covered in condition2 because "give us the rights" the rights is not the sensitive data
    # if "give us" in sentence.strip().lower() or "grant us" in sentence.strip().lower() or " grant, us" in sentence.strip().lower():
    # return "", True,[]

    ###condition 4+: if the sentences contains give + sdk target(give Amplitude), grant us, we return null string, and True which means it will not process nmod
    special_verb = ["grant", "grants", "granted", "give", "gave", "provide", "provided", "providing"]
    for word in special_verb:
        if word in sentence.strip().lower():
            subject_of_special_verb = get_subject(word, sentence, nlp)
            print("=============subject")
            print(subject_of_special_verb)
            for s in subject_of_special_verb:
                if is_first_party(s):
                    return "", True, []

    # phrase_set,_ = extractEntity(sentence,predictor)
    total_verb_list = []
    total_subject_list = []
    string = ""
    for phrase_item in phrase_set:
        verb_list = getVerbBasedOnEntity(phrase_item, sentence, nlp)
        total_verb_list.extend(verb_list)
        for verb in verb_list:
            subject_list = get_subject(verb.text, sentence, nlp)
            total_subject_list.extend(subject_list)

            ### condition 3: filter the verb which not in the collection
            if verb.lemma_.lower() not in collection:
                continue
            else:

                string = string + verb.text + "--->" + phrase_item + "\n"

    #### condition2: check if the the subject of verb is "we", "us"
    subject_candidate = []
    for s in total_subject_list:
        subject_candidate.append(s)
        for synoym in co_reference:
            if s in synoym:
                subject_candidate.extend(synoym)

    flag = False
    for s in list(set(subject_candidate)):
        if is_first_party(s):
            flag = True

    if flag:
        return "", flag, list(set(total_subject_list))
    else:
        return string, flag, list(set(total_subject_list))


def get_subject(verb, sentence, nlp):
    subject_list = []
    doc = nlp(sentence)

    ####obtain subject
    for i in range(0, len(doc)):
        # print(doc[i].text + "--->"  + doc[i].dep_  + "---->" + doc[i].head.text)
        if doc[i].head.text == verb:
            subject_list.append(doc[i].text)
    # print("===================" + verb)
    # print(subject_list)
    return subject_list


### check the noun phrase: the use of the google user data
def assign_nmod_of_sensitive_word(sentence, nlp, standford_nlp, predictor, sensitive_data):
    # _,sensitive_data = extractEntity(sentence,predictor)
    ###condition 1: if the sentence is end with "?", return null string
    if sentence.strip().endswith("?"):
        return ""

    string = ""
    for sensitive_word in sensitive_data:
        nmod = get_word_head(sentence, sensitive_word, standford_nlp)
        if nmod is None:
            continue
        # transfer the nmod from noun to the verb format: disclosure --> disclose
        verb_format = convert(nmod, 'n', 'v')
        verb_list = []
        for v in verb_format:
            if v in collection:
                string = string + nmod + "-->" + v + "--->" + sensitive_word + "\n"
    return string


def assign_verb_and_entity_without_filter(sentence, predictor, nlp, phrase_set):
    # sentence = "We also collect contact information if you choose to upload, sync or import it from a device such as an address book or call log or SMS log history, which we use for things like helping you and others find people you may know and for the other purposes listed below."
    # phrase_set,_ = extractEntity(sentence,predictor)

    string = ""
    for phrase_item in phrase_set:
        verb_list = getVerbBasedOnEntity(phrase_item, sentence, nlp)
        for verb in verb_list:
            string = string + verb.text + "--->" + phrase_item + "\n"

    return string


# check given the subject of verb
def is_first_party(subject):
    # filter_first_subject = ["we", "us","twitter","our","facebook","google","alipay"]
    filter_first_subject = ["tencent", "travelport", "mobileSoft", "we", "us", "twitter", "our", "facebook", "google",
                            "alipay", "edmodo", "pollfish", "adobe", "appnext", "here", "spotify", "uber", "vimeo",
                            "amplitude", "ionicframework", "mixpanel", "seattleclouds", "swmansion", "tiktok",
                            "dropbox", "kakao", "wechat", "line", "linkedin", "pinterest", "vk", "snapchat",
                            "instagram", "zendesk", "squareup", "airmap", "zoom", "gotenna", "vimeo", "fortmatic",
                            "slack", "matterport", "trello", "mindbodyonline", "spotify", "snap","unity","fabric","onesignal","flurry","startapp","applovin","chartboost","firebase","amazon", "paypal","appsflyer","airbnb","mopub","adcolony","vungle"]

    '''
    for item in filter_first_subject:
        if item in subject.lower().split():
            return True
    '''

    if subject.lower() in filter_first_subject:
        return True
    else:
        return False


# determine whether the subject or object of first verb is "we", "us"
'''
def is_first_party(sentence,nlp,predictor):
    flag = False
    try :
        clause = split_clauses(predictor,sentence)
        for c in clause:
            if clause_is_first_party(c,nlp,predictor):
                flag = True
                return flag
    except:
        flag = clause_is_first_party(sentence,nlp,predictor)

    return flag
'''

'''
def clause_is_first_party(sentence,nlp,predictor):
    flag = False
    doc = nlp(sentence)
    verb = [t for t in doc if t.pos_=='VERB']
    filter_first_subject = ["we", "us"]
    if len(verb) > 0:
        first_verb = verb[0]

        ####check first verb
        for i in range(0, len(doc)):
            if doc[i].head.text == first_verb.text and doc[i].text.lower() in filter_first_subject:
                flag = True
                print(doc[i].head.text)
                return flag


        if len(verb) > 1:
            second_verb = verb[1]
            print("the first verb is ")
            print(first_verb)

            print("the second verb is ")
            print(second_verb)

            ####check second verb
            for i in range(0, len(doc)):
                if doc[i].head.text == second_verb.text and doc[i].text.lower() in filter_first_subject:
                    flag = True
                    print(doc[i].head.text)
                    return flag

    else:
        return flag

'''

def getDirectVerb(phrase_item, sentence, nlp):
    doc = nlp(sentence)
    # Merge the noun phrases
    for phrase in list(doc.noun_chunks):
        phrase.merge(phrase.root.tag_, phrase.root.lemma_, phrase.root.ent_type_)

    verb_list = []
    for token in doc:
        if token.text == phrase_item:
            curr = token
            while curr.dep_ != "ROOT" and curr.pos_ != "VERB":
                curr = curr.head
                # print(curr)
            if curr.pos_ == "VERB":
                verb_list.append(curr.text)
                return verb_list[0]
    return []



def getVerbBasedOnEntity(phrase_item, sentence, nlp):
    doc = nlp(sentence)
    # Merge the noun phrases
    for phrase in list(doc.noun_chunks):
        phrase.merge(phrase.root.tag_, phrase.root.lemma_, phrase.root.ent_type_)

    verb_list = []
    for token in doc:
        if token.text == phrase_item:
            curr = token
            while curr.dep_ != "ROOT" and curr.pos_ != "VERB":
                curr = curr.head
                # print(curr)
            if curr.pos_ == "VERB":
                verb_list.append(curr)

    ###extend conjuction verb
    total_verb = set()
    for verb in verb_list:
        total_verb.add(verb)
        find_conj_verb(verb, total_verb)
    return total_verb


## condition 3 find conjuction verb
## for example, Do not modify, translate or delete a portion of the Twitter Content.
## the verb should be modify, translate, delete --> Twitter content
def find_conj_verb(verb, total_verb):
    if verb.dep_ == "conj":
        total_verb.add(verb.head)
        find_conj_verb(verb.head, total_verb)
    else:
        return


def extractEntity(sentence, predictor):
    pharse_list = generate_testFile(sentence)
    testFile = "tmp.tsv"

    write(testFile, predictor)
    to_filemame = "tmp_feature_v1.tsv"
    generate_results(to_filemame)
    sensitive_data = getLabeledWord()
    phrase_set = set()
    for data in sensitive_data:
        for phrase in pharse_list:
            p_list = phrase.split(" ")
            if data in p_list:
                phrase_set.add(phrase)
    # print(phrase_set)
    deleteTmpFile()
    return phrase_set, sensitive_data


def deleteTmpFile():
    del_file1 = "rm -rf tmp.tsv"
    del_file2 = "rm -rf tmp_feature_v1.tsv"
    #del_file3 = "rm -rf /Users/huthvincent/Desktop/paper_works/filter_Sentence_Based_Verb/ner_model/result.txt"
    del_file3 = "rm -rf ../customizeNER/result.txt"
    os.system(del_file1)
    os.system(del_file2)
    os.system(del_file3)


# read the result.txt file and return word whose label is SEC
def getLabeledWord():
    #filename = "/Users/huthvincent/Desktop/paper_works/filter_Sentence_Based_Verb/ner_model/result.txt"
    filename = "../customizeNER/result.txt"
    f = open(filename)
    content = f.readlines()
    f.close()
    sensitive_data = []
    for row in content:
        item_list = row.split("\t")
        # print("==============" + str(len(item_list)))
        # print(item_list)
        # print("\n")
        if len(item_list) < 3:
            continue
        if "SEC" in item_list[2]:
            sensitive_data.append(item_list[0])
    return sensitive_data


def generate_testFile(sentence):
    nlp = spacy.load("en_core_web_sm")
    doc = nlp(sentence)
    sentence_list = [token.text for token in doc]

    # assign O to each word
    label_list = ["O" for i in range(0, len(sentence_list))]
    data = {}
    data["sentence_list"] = sentence_list
    data["label_list"] = label_list
    df = pd.DataFrame(data)
    df.to_csv("tmp.tsv", sep='\t', index=False, header=False, encoding="utf8", mode='w')

    # Merge the noun phrases
    for phrase in list(doc.noun_chunks):
        phrase.merge(phrase.root.tag_, phrase.root.lemma_, phrase.root.ent_type_)
    pharse_list = [phrase.text for phrase in doc]
    # print("====================================================================")
    # print(pharse_list)
    return pharse_list


# invoke trained model and get the results
# parameter: testFile
def generate_results(to_filemame):
    curr = os.getcwd()
    ner_path = "../customizeNER/"
    os.chdir(ner_path)
    cmd = "java -jar ner.jar " + curr +"/"+to_filemame
    os.system(cmd)
    del_file1 = "rm -rf features-true.txt"
    del_file2 = "rm -rf true"
    os.system(del_file1)
    os.system(del_file2)
    os.chdir(curr)


if __name__ == "__main__":
    main()
